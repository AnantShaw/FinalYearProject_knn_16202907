Project Name: k-nn speedup                       

Supervisor Name: Prof. P‡draig CunninghamStudent Name: Anant Kumar Shaw, Student number: 16202907The core objective of this project is to implement an algorithm which is not as aggressive in editing the case-base as algorithms like the CNN, which improves prediction accuracy, but still reduces the case-base to a considerable size to maintain the prediction speed close to the CNN edited case-base.The advanced part of this project involves the implementation of a different Distance Metric to deal with time-series datasets. The project is divided into two parts:1. Core Analysis- This part involves the implementation of the Brute Force k-NN, Ball Tree k-NN and K-D Tree k-NN methods along with the Brute Force k-NN implementation on the edited case-bases from the CNN and CRR case-base editing algorithms on 7 different datasets and comparing the speeds and accuracies of the case-base editing algorithms. The speed comparison of the Brute Force, Ball Tree and K-D Tree algorithms is also reported. The Euclidean Distance Metric is used to compute similarity between two sample cases.2. Advanced Analysis- This part involves the implementation both the case-base editing algorithms (CNN and CRR) using the Dynamic Time Warping Distance metric for time-series datasets. These algorithms are tested on 2 different time-series datasets.A total of 5 Jupyter Notebooks and 1 Python module have been used to carry out the project. They are as follows:1. Dynamic_tw Notebook- This notebook implements the Dynamic Time Warping Distance Metric and Function to create an indices matrix of neighborhood list for each target case, in descending order of similarity.2. Conservative_RR Notebook- This notebook implements the CRR algorithm using both, the Euclidean Distance metric for the Core Analysis, and the Dynamic Time Warping Distance metric for the Advanced Analysis.3. Evaluation_Core Notebook- This notebook implements all the data structure algorithms (Brute Force k-NN, Ball Tree k-NN and K-D Tree k-NN) on the original 7 datasets and Brute Force k-NN again using the 7 edited datasets from the 2 case-base editing algorithms (CNN and CRR) using the Euclidean Distance Metric.4. Evaluation_Advance Notebook- This notebook implements the Brute Force k-NN method on the original 2 datasets and Brute Force k-NN again using the 2 edited datasets from the 2 case-base editing algorithms (CNN and CRR) using the Dynamic Time Warping Distance Metric.5. dataset_loader Python module- This python script loads all the dataset CSV files and converts them to python DataFrames. Necessary data cleaning is also done. 6. FYP Notebook- This notebook is the main notebook for this project where all the other notebooks and module created are imported. All the evaluation functions are run from this notebook, and the data is loaded from the module and passed to the different functions. The results are scaled with respect to the Brute Force k-NN results for simplicity and displayed in this notebook.No extra software apart from Anaconda Navigator is used to run the Jupyter Notebooks. All the necessary packages have been imported in each notebook.  Only the availability to run Jupyter Notebooks is required for this project.In order to run the project and reproduce all the results, the FYP Notebook should be run from beginning till end, and all the results will be displayed, and the new results will replace the old results in the results folder.All the original datasets are stored in the datasets folder of the repository. All the results are stored in the results folder of the repository. All the necessary Notebooks are stored in the repository.